
<!DOCTYPE html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Haochen Wang's homepage.">
  <meta name="keywords" content="Haochen Wang, Haochen, Zhaoxiang Zhang,
                                 SJTU, CASIA,
                                 Deep Learning, Computer Vision,
                                 Segmentation, Semi-Supervised Learning, SSL, Domain Adaptation, UDA, DA">
  <meta name="author" content="Haochen Wang">

  <title>Haochen Wang</title>

  <link rel="stylesheet" type="text/css" href="assets/font-awesome-4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" type="text/css" href="assets/academicons-1.8.6/css/academicons.min.css">
  <!-- <link rel="stylesheet" type="text/css" href="assets/bootstrap-4.3.1-dist/css/bootstrap.min.css"> -->
  <link rel="stylesheet" type="text/css" href="assets/style.css">
  <link rel="icon" type="image/png" href="assets/figures/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="assets/figures/large-icon.png">
</head>
<!-- === Header Ends === -->


<body data-new-gr-c-s-check-loaded="14.1014.0" data-gr-ext-installed="">

<style>
a{text-decoration:none;}
a:link {color:#0066ff;}
a:visited {color:#0066ff;}
a:hover {color:#00ccff;}
a:active {color:#0066ff;}
</style>

<!-- === Homepage Starts === -->
<table width="980px" align="center" border="0">
<tbody>
<tr>


<td></td>  <!-- Leave one column blank on the left. -->
<td valign="top">


<!-- === Avatar Starts === -->
<br>
<table style="font-size: 12pt;" width="100%" border="0">
<tbody>
  <tr>
    <td width="30%">
      <img width="250" src="./figures/me.jpg">
    </td>

    <td>
      <div class="col-xs-12 col-sm-8">
        <h1>
          <strong>Haochen Wang</strong><br>
          </h1>
            <p>
              Institute of Automation, Chinese Academy of Sciences<br>
              Beijing, 100190, P.R.China
            </p>

            <p>Email: <code>wanghaochen2022[at]ia.ac.cn</code></p>

            <p>
                  [<a href="https://github.com/Haochen-Wang409" target="_blank">Github</a>]
                  [<a href="https://scholar.google.com/citations?user=oNlpTdcAAAAJ&hl=en" target="_blank">Google Scholar</a>]
            </p>
      </div>
    </td>
  </tr>
</tbody>
</table>
<!-- === Avatar Ends === -->


<!-- === Biography Starts === -->
<br>
<h2>Biography</h2>
<p style="font-size: 12pt; text-align: justify;">
  <p>
  Haochen Wang is currently a third year Ph.D. student at
  <a href="http://cripac.ia.ac.cn/en/EN/volumn/home.shtml" target="_blank">the Center for Research on Intelligent Perception and Computing</a>,
  <a href="http://mais.ia.ac.cn/" target="_blank">State Key Laboratory of Multimodal Artificial Intelligence Systems</a>,
  <a href="http://english.ia.cas.cn/" target="_blank">Institute of Automation</a>,
  <a href="https://english.cas.cn/" target="_blank">Chinese Academy of Sciences</a>,
  under the supervision of Prof.
  <a href="https://zhaoxiangzhang.net/" target="_blank">Zhaoxiang Zhang</a>.
    
  Before that, he got his Bachelor's degree from
  <a href="https://me.sjtu.edu.cn/en/" target="_blank">the School of Mechanical Engineering</a>,
  <a href="https://en.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a> at June 2022.
  </p>

  <p>
  His research focuses on computer vision and pattern recognition,
  particularly on the following topics:
  <ul>
    <li>Large Multimodal Models</li>
    <li>Label-Efficient Learning</li>
    <li>Unsupervised Visual Representation Learning</li>
  </ul>
  </p>
</p>
<!-- === Biography Ends === -->


<!-- === News Starts === -->
<br>
<h2>News</h2>
<ul style="font-size: 12pt; text-align: justify;">
<table>
<tbody>
  <tr>
    <td width="30%">
      <code>10/2024</code>
    </td>
    <td>
      One paper is accepted by to <strong>ICLR 2025</strong>!
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>10/2024</code>
    </td>
    <td>
      One paper is accepted by <strong>NeurIPS 2024</strong>!
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>10/2024</code>
    </td>
    <td>
      Two papers are accepted by <strong>IJCV</strong>!
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>09/2023</code>
    </td>
    <td>
      One paper is accepted by <strong>NeurIPS 2023</strong>!
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>02/2023</code>
    </td>
    <td>
      Two papers are accepted by <strong>CVPR 2023</strong>!
    </td>
  </tr>
  
  <tr>
    <td width="30%">
      <code>09/2022</code>
    </td>
    <td>
      One paper is accepted by <strong>NeurIPS 2022</strong>!
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>06/2022</code>
    </td>
    <td>
      Join Megvii as a research intern.
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>03/2022</code>
    </td>
    <td>
      One paper is accepted by <strong>CVPR 2022</strong>!
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>06/2021</code>
    </td>
    <td>
      Join SenseTime as a research intern.
    </td>
  </tr>
</tbody>
</table>

</ul>
<!-- === News Ends === -->


<!-- === Publication Starts === -->
<br>
<h2> Publications </h2>
  <strong>First Author / First Co-Author</strong>
<table cellspacing="17">
<tbody>
  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/ross.png">
    </td>
    <td>
      <strong>Reconstructive Visual Instruction Tuning</strong>

      <br><small>
      <strong>Haochen Wang</strong>,
      <a href="https://yexiguafuqihao.github.io/" target="_blank"><font color="#7F7F7F">Anlin Zheng,</font></a>
      <a href="https://scholar.google.com/citations?user=QWemjjQAAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Yucheng Zhao,</font></a>
      <a href="https://scholar.google.com/citations?user=YI0sRroAAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Tiancai Wang,</font></a>
      <a href="https://joker316701882.github.io/" target="_blank"><font color="#7F7F7F">Zheng Ge,</font></a>
      <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Xiangyu Zhang</font></a>
      <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ" target="_blank"><font color="#7F7F7F">Zhaoxiang Zhang</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2410.09575.pdf" target="_blank">Paper (ICLR 2025)</a>]
      [<a href="https://github.com/Haochen-Wang409/ross" target="_blank">Code</a>]
      [<a href="https://huggingface.co/HaochenWang/ross-qwen2-7b" target="_blank">Huggingface Checkpoint</a>]
      </small>

    </td>
  </tr>

  <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/opensatmap.png">
    </td>
    <td>
      <strong>OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction</strong>

      <br><small>
      <a href="https://bjzhb666.github.io/" target="_blank"><font color="#7F7F7F">Hongbo Zhao*,</font></a>
      <a href="https://yexiguafuqihao.github.io/" target="_blank"><font color="#7F7F7F">Lue Fan*,</font></a>
      <a href="http://lue.fan/" target="_blank"><font color="#7F7F7F">Yuntao Chen*,</font></a>
      <strong>Haochen Wang*</strong>,
      <a href="https://scholar.google.com/citations?user=EnvWK20AAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Yuran Yang*,</font></a>
      <a href="" target="_blank"><font color="#7F7F7F">Xiaojuan Jin,</font></a>
      <a href="" target="_blank"><font color="#7F7F7F">Yixin Zhang,</font></a>
      <a href="https://scholar.google.com/citations?user=5hti_r0AAAAJ" target="_blank"><font color="#7F7F7F">Gaofeng Meng,</font></a>
      <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ" target="_blank"><font color="#7F7F7F">Zhaoxiang Zhang</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2410.23278?" target="_blank">Paper (NeurIPS 2024)</a>]
      [<a href="https://opensatmap.github.io/" target="_blank">Project Page</a>]
      [<a href="https://huggingface.co/datasets/z-hb/OpenSatMap" target="_blank">Huggingface Dataset</a>]
      </small>

    </td>
  </tr>

  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/DropPos.png">
    </td>
    <td>
      <strong>DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions</strong>

      <br><small>
      <strong>Haochen Wang</strong>,
      <a href="https://scholar.google.com/citations?hl=en&user=AfK4UcUAAAAJ" target="_blank"><font color="#7F7F7F">Junsong Fan,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=waLCodcAAAAJ" target="_blank"><font color="#7F7F7F">Yuxi Wang,</font></a>
      <a href="https://scholar.google.com/citations?user=VLqzM1wAAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Kaiyou Song,</font></a>
      <font color="#7F7F7F">Tong Wang,</font>
      <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ" target="_blank"><font color="#7F7F7F">Zhaoxiang Zhang</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2309.03576.pdf" target="_blank">Paper (NeurIPS 2023)</a>]
      [<a href="https://github.com/Haochen-Wang409/DropPos" target="_blank">Code</a>]
      </small>

    </td>
  </tr>

  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/hpm.png">
    </td>
    <td>
      <strong>Hard Patches Mining for Masked Image Modeling</strong>

      <br><small>
      <strong>Haochen Wang</strong>,
      <a href="https://scholar.google.com/citations?user=VLqzM1wAAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Kaiyou Song,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=AfK4UcUAAAAJ" target="_blank"><font color="#7F7F7F">Junsong Fan,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=waLCodcAAAAJ" target="_blank"><font color="#7F7F7F">Yuxi Wang,</font></a>
      <a href="https://openreview.net/profile?id=~Jin_Xie9" target="_blank"><font color="#7F7F7F">Jin Xie,</font></a>
      <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ" target="_blank"><font color="#7F7F7F">Zhaoxiang Zhang</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2304.05919.pdf" target="_blank">Paper (CVPR 2023)</a>]
      [<a href="https://github.com/Haochen-Wang409/HPM" target="_blank">Code</a>]
      </small>

    </td>
  </tr>

  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/t2s-da.png">
    </td>
    <td>
      <strong>Pulling Target to Source: A New Perspective on Domain Adaptive Semantic Segmentation</strong>

      <br><small>
      <strong>Haochen Wang</strong>,
      <a href="https://shenyujun.github.io/" target="_blank"><font color="#7F7F7F">Yujun Shen,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=vSM7n_UAAAAJ" target="_blank"><font color="#7F7F7F">Jingjing Fei,</font></a>
      <a href="https://bigballon.github.io/" target="_blank"><font color="#7F7F7F">Wei Li,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=dg1JyaUAAAAJ" target="_blank"><font color="#7F7F7F">Liwei Wu,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=waLCodcAAAAJ" target="_blank"><font color="#7F7F7F">Yuxi Wang,</font></a>
      <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ" target="_blank"><font color="#7F7F7F">Zhaoxiang Zhang</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2306.02314" target="_blank">Paper (IJCV)</a>]
      [<a href="https://github.com/Haochen-Wang409/StyleTransfer" target="_blank">Code</a>]
      </small>

    </td>
  </tr>

  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/u2plp.png">
    </td>
    <td>
      <strong>Using Unreliable Pseudo-Labels for Label-Efficient Semantic Segmentation</strong>

      <br><small>
      <strong>Haochen Wang</strong>,
      <a href="https://scholar.google.com/citations?user=O8zH6pYAAAAJ&hl=en" target="_blank"><font color="#7F7F7F">Yuchao Wang,</font></a>
      <a href="https://shenyujun.github.io/" target="_blank"><font color="#7F7F7F">Yujun Shen,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=AfK4UcUAAAAJ" target="_blank"><font color="#7F7F7F">Junsong Fan,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=waLCodcAAAAJ" target="_blank"><font color="#7F7F7F">Yuxi Wang,</font></a>
      <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ" target="_blank"><font color="#7F7F7F">Zhaoxiang Zhang</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2306.02314" target="_blank">Paper (IJCV)</a>]
      [<a href="https://github.com/Haochen-Wang409/U2PL" target="_blank">Code</a>]
      </small>

    </td>
  </tr>

  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/u2pl.png">
    </td>
    <td>
      <strong>Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels</strong>

      <br><small>
      <font color="#7F7F7F">Yuchao Wang*,</font>
      <strong>Haochen Wang*</strong>,
      <a href="https://shenyujun.github.io/" target="_blank"><font color="#7F7F7F">Yujun Shen,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=vSM7n_UAAAAJ" target="_blank"><font color="#7F7F7F">Jingjing Fei,</font></a>
      <a href="https://bigballon.github.io/" target="_blank"><font color="#7F7F7F">Wei Li,</font></a>
      <font color="#7F7F7F">Guoqiang Jin,</font>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=dg1JyaUAAAAJ" target="_blank"><font color="#7F7F7F">Liwei Wu,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=1c9oQNMAAAAJ" target="_blank"><font color="#7F7F7F">Rui Zhao,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=MGZyMf4AAAAJ" target="_blank"><font color="#7F7F7F">Xinyi Le</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2203.03884.pdf" target="_blank">Paper (CVPR 2022)</a>]
      [<a href="https://github.com/Haochen-Wang409/U2PL" target="_blank">Code</a>]
      </small>

    </td>
  </tr>
</tbody>
</table>

  <strong>Co-Author</strong>

<table cellspacing="17">
<tbody>
 <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/blv.png">
    </td>
    <td>
      <strong>Balancing Logit Variation for Long-tailed Semantic Segmentation</strong>

      <br><small>
      <font color="#7F7F7F">Yuchao Wang,</font>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=vSM7n_UAAAAJ" target="_blank"><font color="#7F7F7F">Jingjing Fei,</font></a>
      <strong>Haochen Wang</strong>,
      <a href="https://bigballon.github.io/" target="_blank"><font color="#7F7F7F">Wei Li,</font></a>
      <a href="https://openreview.net/profile?id=~Tianpeng_Bao1" target="_blank"><font color="#7F7F7F">Tianpeng Bao,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=dg1JyaUAAAAJ" target="_blank"><font color="#7F7F7F">Liwei Wu,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=1c9oQNMAAAAJ" target="_blank"><font color="#7F7F7F">Rui Zhao,</font></a>
      <a href="https://shenyujun.github.io/" target="_blank"><font color="#7F7F7F">Yujun Shen</font></a>

      <br>
      [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Balancing_Logit_Variation_for_Long-Tailed_Semantic_Segmentation_CVPR_2023_paper.pdf" target="_blank">Paper (CVPR 2023)</a>]
      [<a href="https://github.com/grantword8/BLV" target="_blank">Code</a>]
      </small>

    </td>
  </tr>

  <tr>
    <td width="25%">
      <img style="width: 100%; max-height: 150px; object-fit: cover;" src="./figures/projects/fst.png">
    </td>
    <td>
      <strong>Learning from Future: A Novel Self-Training Framework for Semantic Segmentation</strong>

      <br><small>
      <a href="https://usr922.github.io/" target="_blank"><font color="#7F7F7F">Ye Du,</font></a>
      <a href="https://shenyujun.github.io/" target="_blank"><font color="#7F7F7F">Yujun Shen,</font></a>
      <strong>Haochen Wang</strong>,
      <a href="https://scholar.google.com.hk/citations?hl=en&user=vSM7n_UAAAAJ" target="_blank"><font color="#7F7F7F">Jingjing Fei,</font></a>
      <a href="https://bigballon.github.io/" target="_blank"><font color="#7F7F7F">Wei Li,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=dg1JyaUAAAAJ" target="_blank"><font color="#7F7F7F">Liwei Wu,</font></a>
      <a href="https://scholar.google.com.hk/citations?hl=en&user=1c9oQNMAAAAJ" target="_blank"><font color="#7F7F7F">Rui Zhao,</font></a>
      <a href="https://scholar.google.com/citations?user=Ug8JzsAAAAAJ&hl=en&oi=ao", target="_blank"><font color="#7F7F7F">Zehua Fu,</font></a>
      <a href="https://scholar.google.com/citations?hl=en&user=HsLdRZYAAAAJ" target="_blank"><font color="#7F7F7F">Qingjie Liu</font></a>

      <br>
      [<a href="https://arxiv.org/pdf/2209.06993.pdf" target="_blank">Paper (NeurIPS 2022)</a>]
      [<a href="https://github.com/usr922/FST" target="_blank">Code</a>]
      </small>

    </td>
  </tr>
</tbody>
</table>
<!-- === Publication Ends === -->


<!-- === Experiences Starts === -->
<br>
<h2>Experiences</h2>
<ul style="font-size: 12pt; text-align: justify;">
<table>
<tbody>
  <tr>
    <td width="30%">
      <code>09/2023 - Present</code>
    </td>
    <td>
      Research intern at <a href="https://megvii.com/" target="_blank">Megvii</a>
      under the supervision of
      <a href="https://scholar.google.com/citations?user=YI0sRroAAAAJ&hl=en&oi=ao" target="_blank">Tiancai Wang</a>
      and
      <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en&oi=ao" target="_blank">Xiangyu Zhang</a>.
    </td>
  </tr>

  <tr>
    <td width="30%">
      <code>06/2022 - 09/2022</code>
    </td>
    <td>
      Research intern at <a href="https://megvii.com/" target="_blank">Megvii</a>
      under the supervision of 
      <a href="https://scholar.google.com/citations?user=VLqzM1wAAAAJ&hl=en&oi=ao" target="_blank">Kaiyou Song</a>.
    </td>
  </tr>
  
  <tr>
    <td width="30%">
      <code>06/2021 - 04/2022</code>
    </td>
    <td>
      Research intern at <a href="https://www.sensetime.com/en" target="_blank">SenseTime</a>
      under the supervision of
      <a href="https://scholar.google.com.hk/citations?hl=en&user=dg1JyaUAAAAJ" target="_blank">Liwei Wu</a> and
      <a href="https://scholar.google.com.hk/citations?hl=en&user=1c9oQNMAAAAJ" target="_blank">Rui Zhao</a>.
    </td>
  </tr>
</tbody>
</table>
</ul>
<!-- === Experiences Ends === -->


<!-- === Academic Service Starts === -->
<br>
<h2>Academic Service</h2>
<p style="font-size: 12pt; text-align: justify;">
  <strong>Conference Reviewer</strong>:
  <ul>
  <li> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023, 2024, 2025 </li>
  <li> IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023 </li>
  <li> European Conference on Computer Vision (<strong>ECCV</strong>), 2024 </li>
  <li> Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023, 2024 </li>
  <li> International Conference on Learning Representations (<strong>ICLR</strong>), 2024, 2025 </li>
  <li> International Conference on Machine Learning (<strong>ICML</strong>), 2024 </li>
  <li> AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2025 </li>
  </ul>
</p>

  <p style="font-size: 12pt; text-align: justify;">
  <strong>Journal Reviewer</strong>:
  <ul>
  <li> IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>) </li>
  <li> International Journal of Computer Vision (<strong>IJCV</strong>)</li>
  <li> IEEE Transactions on Multimedia (<strong>TMM</strong>) </li>
  <li> IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>)</li>
  </ul>
</p>
<!-- === Academic Service Ends === -->  


<!-- === Awards Starts === -->
<br>
<h2>Awards</h2>
<ul style="font-size: 12pt; text-align: justify;">
<table>
<tbody>
  <tr>
    <td width="15%">
      <code>04/2022</code>
    </td>
    <td>
      Shanghai Outstanding Graduates (top 0.5%)
    </td>
  </tr>

  <tr>
    <td width="15%">
      <code>12/2021</code>
    </td>
    <td>
      China National Scholarship (top 0.2%)
    </td>
  </tr>

  <tr>
    <td width="15%">
      <code>04/2021</code>
    </td>
    <td>
      Outstanding Winner of
      Mathematical Contest in Modeling (top 0.1%)
    </td>
  </tr>

  <tr>
    <td width="15%">
      <code>04/2021</code>
    </td>
    <td>
      Tyacht Scholarship (top 2%)
    </td>
  </tr>

  <tr>
    <td width="15%">
      <code>11/2020</code>
    </td>
    <td>
      Second Price in China National Mathematical Contest in Modeling
    </td>
  </tr>

  <tr>
    <td width="15%">
      <code>01/2020</code>
    </td>
    <td>
      Tang Lixin Scholarship (top 1%)
    </td>
  </tr>

  <tr>
    <td width="15%">
      <code>12/2017</code>
    </td>
    <td>
      Second Price in National Olympiad in Informatics in Zhejiang Province
    </td>
  </tr>

</tbody>
</table>
</ul>
<!-- === Awards Ends === -->


</td>
</tr>
</tbody>
</table>
<!-- === Homepage Ends === -->


<!-- Visitor Traffic -->
<script type='text/javascript' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=0&t=tt&d=ZoDwnZl26icJVfh0W6PRaWJC8UIjGa9CyHu2YaqgaRI&co=ffffff&ct=ffffff&cmo=ffffff&cmn=ffffff'></script>


</body>
</html>

